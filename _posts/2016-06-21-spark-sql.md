---
layout: post
title: spark sql
tags:  [Spark]
categories: [Spark]
author: mingtian
excerpt: "Spark"
---


### spark sql

Spark Version 1.4

Attribute  vs StructType

StructType 相当于关系型数据库中的Schema,SturctField 相当于对每个列的定义,因此StructFiled 需要定义名称,类型,是否为空
StructType 中包含多个StructField. 两者都是case class

Attribute 相当于Expression -> TreeNode


```
val inputDF = sqlc.read.json(logFile)
```

sqlContext.read.json(path) 会调用具体的load 方法，找到 json 格式对应的DefaultDataSource,DefaultDataSource
继承RelationProvider,实现创建Relation的方法,Relation 用来提供schema的获取方法。由于json 使用了外部数据源的方式，
在JsonRelation 中还有定义数据的scan 方式具体的数据处理在JsonRDD 中实现, scan 返回的是RDD[Row],JsonRelation 使用 TableScan , CatalystScan。最后生成DataFrame 参数是 Relation 和SQLContext。

scan 有 4种BaseRelation,分为TableScan, PrunedScan, PrunedFilterScan, CatalystScan。 最后会把生成的Relation
传递到LogicalRelation 中。

   + TableScan：
          没有任何操作，类似全表扫描 select * from xxx
   + PrunedScan：
          传入指定列的数组 requiredColumns,列裁剪去掉不需要的列 select c1,c2 from xxx 
   + PrunedFilterScan：
          在列裁剪的基础上，并且加入Filter机制，在加载数据也的时候就进行过滤，而不是在客户端请求返回时做Filter
   + CatalystScan：
           Catalyst的支持传入expressions来进行Scan。支持列裁剪和Filter

```
val query = inputDF.select(inputDF("age"), inputDF("department"))
            .filter(inputDF("age").<(31))
            .filter(inputDF("age") < 29)
            .groupBy("department", "age")
            .count()
```
```
 @inline private implicit def logicalPlanToDataFrame(logicalPlan: LogicalPlan): DataFrame = {
    new DataFrame(sqlContext, logicalPlan)
  }
```
生成 DataFrame后,每次调用操作符(select,filter,groupBy) 都会创建新的DataFrame,这个过程通过隐式转换,此时每个child 对应的LogicalPlan 都是上一个DataFrame对应并且解析过的。每次创建新的DataFrame 都会解析新的LogicalPlan 具体的使用Analyszer 父类 RuleExecutor 中的execute 方法。比如判断查询的列，表是否存在(ResolveRelations)

生成的LogicalPlan:

```
Limit 20
 Aggregate [department#1L], [department#1L,COUNT(1) AS count#3L]
  Filter (age#0L < CAST(29, LongType))
   Filter (age#0L < CAST(31, LongType))
    Project [age#0L,department#1L]
     Relation[age#0L,department#1L,name#2] org.apache.spark.sql.json.JSONRelation@d7964b01
```


```
 query.show(20)
```

调用 QueryExecution.executedPlan 把 LogicalPlan 转换成可以执行的 PhysicalPlan
通过 Optimizer(DefaultOptimizer) 中定义的规则来处理,如 CombineFilters 用来合并同一个DataFrame 中的多个Filter 操作

生成的OptimizedPlan:

```
Limit 20
 Aggregate [department#1L,age#0L], [department#1L,age#0L,COUNT(1) AS count#3L]
  Project [age#0L,department#1L]
   Filter ((age#0L < 31) && (age#0L < 29))
    Relation[age#0L,department#1L,name#2] org.apache.spark.sql.json.JSONRelation@d7964b01
```

通过SparkPlanner 把优化后的LogicalPlan 通过 Strategy 转换成 PhysicalPlan 返回第一个

生成的SparkPlan

```
 Limit 20
 Aggregate false, [department#1L,age#0L], [department#1L,age#0L,Coalesce(SUM(PartialCount#5L),0) AS count#3L]
  Aggregate true, [department#1L,age#0L], [department#1L,age#0L,COUNT(1) AS PartialCount#5L]
   Filter ((age#0L < 31) && (age#0L < 29))
    PhysicalRDD [age#0L,department#1L], MapPartitionsRDD[6] at show at TestJsonSQLRealation.scala:28

```

最后通过 prepareExecution(继承Rule) 确定分区数

```
Limit 20
 Aggregate false, [department#1L,age#0L], [department#1L,age#0L,Coalesce(SUM(PartialCount#5L),0) AS count#3L]
  Exchange (HashPartitioning 3)
   Aggregate true, [department#1L,age#0L], [department#1L,age#0L,COUNT(1) AS PartialCount#5L]
    Filter ((age#0L < 31) && (age#0L < 29))
     PhysicalRDD [age#0L,department#1L], MapPartitionsRDD[6] at show at TestJsonSQLRealation.scala:26
```

得到最后的PhysicalPlan 后，调用 executeCollect() 开始执行

